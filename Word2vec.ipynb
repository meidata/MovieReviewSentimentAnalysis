{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (34092, 3) ..\n",
      "\n",
      "test data size: (14613, 3) ..\n",
      "\n",
      "Unlabeled train data size : (60797, 2) ..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup \n",
    "import matplotlib.pyplot as plt\n",
    "from reviewsData import * \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "pd.set_option(\"display.max_colwidth\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sentencesToWords \n",
    "lower all the case in the sentences\n",
    "and split by space\n",
    "'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sentencesToWords(movieReviews,remove_stopwords = False):\n",
    "    movieReviews = movieReviews.lower()\n",
    "    movieReviews = re.sub(\"[^a-zA-Z]\",' ',movieReviews)\n",
    "    movieReviews = movieReviews.split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        movieReviews = [w for w in movieReviews if not w in stops]\n",
    "    return movieReviews\n",
    "\n",
    "'''\n",
    "reviewsToListsOfWords\n",
    "separate reviews by sentences\n",
    "and separate sentencs by words\n",
    "final outcome: list of multiple lists  [[],[],[],[]]\n",
    "each lists is a sentences seperated by words\n",
    "'''\n",
    "\n",
    "def reviewsToListsOfWords(review,remove_stopwords=False):\n",
    "    raw_sentences = sent_tokenize(review.strip())\n",
    "    \n",
    "    \n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(sentencesToWords(raw_sentence,remove_stopwords))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 70.96953701972961 seconds..\n",
      "\n",
      "1337771\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "'''\n",
    "length of sentences == lenght of the reviews\n",
    "each lists of sentences = mulitiple lists of sentencs in one reviews\n",
    "'''\n",
    "\n",
    "tic()\n",
    "for review in train['reviews']:\n",
    "    sentences += reviewsToListsOfWords(review)\n",
    "\n",
    "\n",
    "for review in unlabeled_reviews['reviews']:\n",
    "    sentences += reviewsToListsOfWords(review)\n",
    "    \n",
    "toc()\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Unigram Word2Vec model\n",
    "'''\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print('Training model...\\n')\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers,size=num_features,min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cup'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child cup\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.8679860830307007),\n",
       " ('atrocious', 0.7201763391494751),\n",
       " ('horrendous', 0.7171976566314697),\n",
       " ('awful', 0.6972880363464355),\n",
       " ('horrid', 0.6694691181182861),\n",
       " ('bad', 0.6263217329978943),\n",
       " ('poor', 0.607570230960846),\n",
       " ('appalling', 0.5665920972824097),\n",
       " ('abysmal', 0.5638487339019775),\n",
       " ('great', 0.5550647974014282)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.7680087089538574),\n",
       " ('good', 0.7461056709289551),\n",
       " ('wonderful', 0.7308462858200073),\n",
       " ('fantastic', 0.7109869718551636),\n",
       " ('superb', 0.7077364921569824),\n",
       " ('phenomenal', 0.7056679725646973),\n",
       " ('fabulous', 0.702741265296936),\n",
       " ('amazing', 0.6713088750839233),\n",
       " ('excellent', 0.6678848266601562),\n",
       " ('brilliant', 0.6510426998138428)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'meiyi' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73c333b938a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meiyi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'meiyi' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar(\"meiyi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16034, 300)\n"
     ]
    }
   ],
   "source": [
    "print(type(model.wv.syn0))\n",
    "print(model.wv.syn0.shape)\n",
    "'''\n",
    "The number of rows in syn0 is the number of words in the model's vocabulary, \n",
    "and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  \n",
    "Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word_list in words:\n",
    "        for word in word_list:\n",
    "            if word in index2word_set: \n",
    "                nwords = nwords + 1.\n",
    "                featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       \n",
    "       # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3  \n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"reviews\"]:\n",
    "    clean_train_reviews.append(reviewsToListsOfWords(review,remove_stopwords = True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"reviews\"]:\n",
    "    clean_test_reviews.append(reviewsToListsOfWords(review,remove_stopwords = True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-25 22:37:16,524 : INFO : collecting all words and their counts\n",
      "2017-02-25 22:37:16,525 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2017-02-25 22:37:16,885 : INFO : PROGRESS: at sentence #10000, processed 190969 words and 99926 word types\n",
      "2017-02-25 22:37:17,235 : INFO : PROGRESS: at sentence #20000, processed 383963 words and 166808 word types\n",
      "2017-02-25 22:37:17,596 : INFO : PROGRESS: at sentence #30000, processed 575183 words and 222432 word types\n",
      "2017-02-25 22:37:17,940 : INFO : PROGRESS: at sentence #40000, processed 765030 words and 272411 word types\n",
      "2017-02-25 22:37:18,294 : INFO : PROGRESS: at sentence #50000, processed 955638 words and 318734 word types\n",
      "2017-02-25 22:37:18,685 : INFO : PROGRESS: at sentence #60000, processed 1151213 words and 364279 word types\n",
      "2017-02-25 22:37:19,055 : INFO : PROGRESS: at sentence #70000, processed 1342997 words and 405189 word types\n",
      "2017-02-25 22:37:19,425 : INFO : PROGRESS: at sentence #80000, processed 1537413 words and 445102 word types\n",
      "2017-02-25 22:37:19,877 : INFO : PROGRESS: at sentence #90000, processed 1733439 words and 484881 word types\n",
      "2017-02-25 22:37:20,221 : INFO : PROGRESS: at sentence #100000, processed 1922626 words and 520411 word types\n",
      "2017-02-25 22:37:20,558 : INFO : PROGRESS: at sentence #110000, processed 2114163 words and 555736 word types\n",
      "2017-02-25 22:37:20,919 : INFO : PROGRESS: at sentence #120000, processed 2313714 words and 591596 word types\n",
      "2017-02-25 22:37:21,243 : INFO : PROGRESS: at sentence #130000, processed 2506028 words and 624120 word types\n",
      "2017-02-25 22:37:21,580 : INFO : PROGRESS: at sentence #140000, processed 2697228 words and 656057 word types\n",
      "2017-02-25 22:37:21,923 : INFO : PROGRESS: at sentence #150000, processed 2889068 words and 687396 word types\n",
      "2017-02-25 22:37:22,296 : INFO : PROGRESS: at sentence #160000, processed 3078215 words and 717311 word types\n",
      "2017-02-25 22:37:22,646 : INFO : PROGRESS: at sentence #170000, processed 3270897 words and 747381 word types\n",
      "2017-02-25 22:37:23,021 : INFO : PROGRESS: at sentence #180000, processed 3468641 words and 778175 word types\n",
      "2017-02-25 22:37:23,371 : INFO : PROGRESS: at sentence #190000, processed 3658245 words and 805754 word types\n",
      "2017-02-25 22:37:23,721 : INFO : PROGRESS: at sentence #200000, processed 3845539 words and 832985 word types\n",
      "2017-02-25 22:37:24,086 : INFO : PROGRESS: at sentence #210000, processed 4040780 words and 861718 word types\n",
      "2017-02-25 22:37:24,453 : INFO : PROGRESS: at sentence #220000, processed 4238122 words and 890180 word types\n",
      "2017-02-25 22:37:24,823 : INFO : PROGRESS: at sentence #230000, processed 4435230 words and 918819 word types\n",
      "2017-02-25 22:37:25,161 : INFO : PROGRESS: at sentence #240000, processed 4621634 words and 944196 word types\n",
      "2017-02-25 22:37:25,524 : INFO : PROGRESS: at sentence #250000, processed 4819015 words and 972621 word types\n",
      "2017-02-25 22:37:25,873 : INFO : PROGRESS: at sentence #260000, processed 5010933 words and 1002167 word types\n",
      "2017-02-25 22:37:26,221 : INFO : PROGRESS: at sentence #270000, processed 5201621 words and 1030999 word types\n",
      "2017-02-25 22:37:26,570 : INFO : PROGRESS: at sentence #280000, processed 5391608 words and 1058636 word types\n",
      "2017-02-25 22:37:26,922 : INFO : PROGRESS: at sentence #290000, processed 5580522 words and 1086340 word types\n",
      "2017-02-25 22:37:27,274 : INFO : PROGRESS: at sentence #300000, processed 5771388 words and 1113636 word types\n",
      "2017-02-25 22:37:27,624 : INFO : PROGRESS: at sentence #310000, processed 5964736 words and 1140930 word types\n",
      "2017-02-25 22:37:27,967 : INFO : PROGRESS: at sentence #320000, processed 6154462 words and 1167432 word types\n",
      "2017-02-25 22:37:28,320 : INFO : PROGRESS: at sentence #330000, processed 6346544 words and 1194547 word types\n",
      "2017-02-25 22:37:28,677 : INFO : PROGRESS: at sentence #340000, processed 6536792 words and 1220338 word types\n",
      "2017-02-25 22:37:29,026 : INFO : PROGRESS: at sentence #350000, processed 6732110 words and 1245884 word types\n",
      "2017-02-25 22:37:29,380 : INFO : PROGRESS: at sentence #360000, processed 6922193 words and 1270509 word types\n",
      "2017-02-25 22:37:29,737 : INFO : PROGRESS: at sentence #370000, processed 7109017 words and 1294716 word types\n",
      "2017-02-25 22:37:30,087 : INFO : PROGRESS: at sentence #380000, processed 7297855 words and 1318159 word types\n",
      "2017-02-25 22:37:30,440 : INFO : PROGRESS: at sentence #390000, processed 7487249 words and 1342299 word types\n",
      "2017-02-25 22:37:30,794 : INFO : PROGRESS: at sentence #400000, processed 7678422 words and 1366345 word types\n",
      "2017-02-25 22:37:31,146 : INFO : PROGRESS: at sentence #410000, processed 7865726 words and 1389773 word types\n",
      "2017-02-25 22:37:31,604 : INFO : PROGRESS: at sentence #420000, processed 8059943 words and 1413799 word types\n",
      "2017-02-25 22:37:31,954 : INFO : PROGRESS: at sentence #430000, processed 8250764 words and 1437527 word types\n",
      "2017-02-25 22:37:32,316 : INFO : PROGRESS: at sentence #440000, processed 8440334 words and 1460120 word types\n",
      "2017-02-25 22:37:32,673 : INFO : PROGRESS: at sentence #450000, processed 8634251 words and 1483421 word types\n",
      "2017-02-25 22:37:33,030 : INFO : PROGRESS: at sentence #460000, processed 8822870 words and 1505892 word types\n",
      "2017-02-25 22:37:33,382 : INFO : PROGRESS: at sentence #470000, processed 9009782 words and 1527931 word types\n",
      "2017-02-25 22:37:33,745 : INFO : PROGRESS: at sentence #480000, processed 9207571 words and 1539591 word types\n",
      "2017-02-25 22:37:34,111 : INFO : PROGRESS: at sentence #490000, processed 9412516 words and 1547643 word types\n",
      "2017-02-25 22:37:34,471 : INFO : PROGRESS: at sentence #500000, processed 9604044 words and 1558717 word types\n",
      "2017-02-25 22:37:34,817 : INFO : PROGRESS: at sentence #510000, processed 9796811 words and 1563936 word types\n",
      "2017-02-25 22:37:35,199 : INFO : PROGRESS: at sentence #520000, processed 10008653 words and 1583396 word types\n",
      "2017-02-25 22:37:35,568 : INFO : PROGRESS: at sentence #530000, processed 10213568 words and 1597441 word types\n",
      "2017-02-25 22:37:35,908 : INFO : PROGRESS: at sentence #540000, processed 10401602 words and 1609854 word types\n",
      "2017-02-25 22:37:36,274 : INFO : PROGRESS: at sentence #550000, processed 10604594 words and 1624720 word types\n",
      "2017-02-25 22:37:36,604 : INFO : PROGRESS: at sentence #560000, processed 10782508 words and 1642822 word types\n",
      "2017-02-25 22:37:36,947 : INFO : PROGRESS: at sentence #570000, processed 10964045 words and 1659091 word types\n",
      "2017-02-25 22:37:37,299 : INFO : PROGRESS: at sentence #580000, processed 11155328 words and 1675701 word types\n",
      "2017-02-25 22:37:37,656 : INFO : PROGRESS: at sentence #590000, processed 11349218 words and 1693298 word types\n",
      "2017-02-25 22:37:38,038 : INFO : PROGRESS: at sentence #600000, processed 11560679 words and 1713561 word types\n",
      "2017-02-25 22:37:38,403 : INFO : PROGRESS: at sentence #610000, processed 11755700 words and 1730416 word types\n",
      "2017-02-25 22:37:38,723 : INFO : PROGRESS: at sentence #620000, processed 11933066 words and 1735880 word types\n",
      "2017-02-25 22:37:39,069 : INFO : PROGRESS: at sentence #630000, processed 12126334 words and 1752286 word types\n",
      "2017-02-25 22:37:39,411 : INFO : PROGRESS: at sentence #640000, processed 12306709 words and 1766703 word types\n",
      "2017-02-25 22:37:39,733 : INFO : PROGRESS: at sentence #650000, processed 12480488 words and 1777507 word types\n",
      "2017-02-25 22:37:40,072 : INFO : PROGRESS: at sentence #660000, processed 12665560 words and 1782969 word types\n",
      "2017-02-25 22:37:40,443 : INFO : PROGRESS: at sentence #670000, processed 12860315 words and 1801938 word types\n",
      "2017-02-25 22:37:40,799 : INFO : PROGRESS: at sentence #680000, processed 13054436 words and 1819412 word types\n",
      "2017-02-25 22:37:41,192 : INFO : PROGRESS: at sentence #690000, processed 13263919 words and 1840312 word types\n",
      "2017-02-25 22:37:41,539 : INFO : PROGRESS: at sentence #700000, processed 13448542 words and 1853002 word types\n",
      "2017-02-25 22:37:41,899 : INFO : PROGRESS: at sentence #710000, processed 13645876 words and 1863216 word types\n",
      "2017-02-25 22:37:42,233 : INFO : PROGRESS: at sentence #720000, processed 13822784 words and 1879418 word types\n",
      "2017-02-25 22:37:42,571 : INFO : PROGRESS: at sentence #730000, processed 14008627 words and 1896242 word types\n",
      "2017-02-25 22:37:42,941 : INFO : PROGRESS: at sentence #740000, processed 14208431 words and 1907021 word types\n",
      "2017-02-25 22:37:43,297 : INFO : PROGRESS: at sentence #750000, processed 14402490 words and 1918065 word types\n",
      "2017-02-25 22:37:43,633 : INFO : PROGRESS: at sentence #760000, processed 14581151 words and 1930090 word types\n",
      "2017-02-25 22:37:43,976 : INFO : PROGRESS: at sentence #770000, processed 14771178 words and 1944638 word types\n",
      "2017-02-25 22:37:44,322 : INFO : PROGRESS: at sentence #780000, processed 14958749 words and 1960659 word types\n",
      "2017-02-25 22:37:44,678 : INFO : PROGRESS: at sentence #790000, processed 15152188 words and 1970523 word types\n",
      "2017-02-25 22:37:45,015 : INFO : PROGRESS: at sentence #800000, processed 15332240 words and 1980740 word types\n",
      "2017-02-25 22:37:45,361 : INFO : PROGRESS: at sentence #810000, processed 15519555 words and 1992142 word types\n",
      "2017-02-25 22:37:45,750 : INFO : PROGRESS: at sentence #820000, processed 15725104 words and 2011242 word types\n",
      "2017-02-25 22:37:46,083 : INFO : PROGRESS: at sentence #830000, processed 15908143 words and 2031803 word types\n",
      "2017-02-25 22:37:46,435 : INFO : PROGRESS: at sentence #840000, processed 16093765 words and 2052010 word types\n",
      "2017-02-25 22:37:46,794 : INFO : PROGRESS: at sentence #850000, processed 16288258 words and 2071266 word types\n",
      "2017-02-25 22:37:47,146 : INFO : PROGRESS: at sentence #860000, processed 16483102 words and 2090827 word types\n",
      "2017-02-25 22:37:47,485 : INFO : PROGRESS: at sentence #870000, processed 16670601 words and 2107148 word types\n",
      "2017-02-25 22:37:47,840 : INFO : PROGRESS: at sentence #880000, processed 16862691 words and 2124260 word types\n",
      "2017-02-25 22:37:48,183 : INFO : PROGRESS: at sentence #890000, processed 17048547 words and 2140859 word types\n",
      "2017-02-25 22:37:48,561 : INFO : PROGRESS: at sentence #900000, processed 17249108 words and 2158934 word types\n",
      "2017-02-25 22:37:48,927 : INFO : PROGRESS: at sentence #910000, processed 17446908 words and 2176782 word types\n",
      "2017-02-25 22:37:49,308 : INFO : PROGRESS: at sentence #920000, processed 17652737 words and 2197162 word types\n",
      "2017-02-25 22:37:49,685 : INFO : PROGRESS: at sentence #930000, processed 17850858 words and 2208980 word types\n",
      "2017-02-25 22:37:50,044 : INFO : PROGRESS: at sentence #940000, processed 18041034 words and 2214925 word types\n",
      "2017-02-25 22:37:50,395 : INFO : PROGRESS: at sentence #950000, processed 18233232 words and 2221932 word types\n",
      "2017-02-25 22:37:50,741 : INFO : PROGRESS: at sentence #960000, processed 18423739 words and 2226924 word types\n",
      "2017-02-25 22:37:51,114 : INFO : PROGRESS: at sentence #970000, processed 18619954 words and 2241744 word types\n",
      "2017-02-25 22:37:51,471 : INFO : PROGRESS: at sentence #980000, processed 18815169 words and 2246919 word types\n",
      "2017-02-25 22:37:51,869 : INFO : PROGRESS: at sentence #990000, processed 19029848 words and 2257822 word types\n",
      "2017-02-25 22:37:52,223 : INFO : PROGRESS: at sentence #1000000, processed 19223351 words and 2269855 word types\n",
      "2017-02-25 22:37:52,589 : INFO : PROGRESS: at sentence #1010000, processed 19424745 words and 2280682 word types\n",
      "2017-02-25 22:37:52,942 : INFO : PROGRESS: at sentence #1020000, processed 19616162 words and 2297982 word types\n",
      "2017-02-25 22:37:53,287 : INFO : PROGRESS: at sentence #1030000, processed 19807174 words and 2314392 word types\n",
      "2017-02-25 22:37:53,643 : INFO : PROGRESS: at sentence #1040000, processed 20000712 words and 2330981 word types\n",
      "2017-02-25 22:37:54,012 : INFO : PROGRESS: at sentence #1050000, processed 20202707 words and 2349405 word types\n",
      "2017-02-25 22:37:54,353 : INFO : PROGRESS: at sentence #1060000, processed 20387004 words and 2356083 word types\n",
      "2017-02-25 22:37:54,727 : INFO : PROGRESS: at sentence #1070000, processed 20593994 words and 2370122 word types\n",
      "2017-02-25 22:37:55,073 : INFO : PROGRESS: at sentence #1080000, processed 20780115 words and 2383563 word types\n",
      "2017-02-25 22:37:55,417 : INFO : PROGRESS: at sentence #1090000, processed 20958377 words and 2388105 word types\n",
      "2017-02-25 22:37:55,773 : INFO : PROGRESS: at sentence #1100000, processed 21142813 words and 2404221 word types\n",
      "2017-02-25 22:37:56,131 : INFO : PROGRESS: at sentence #1110000, processed 21323564 words and 2422904 word types\n",
      "2017-02-25 22:37:56,478 : INFO : PROGRESS: at sentence #1120000, processed 21508294 words and 2440093 word types\n",
      "2017-02-25 22:37:56,837 : INFO : PROGRESS: at sentence #1130000, processed 21698298 words and 2456509 word types\n",
      "2017-02-25 22:37:57,212 : INFO : PROGRESS: at sentence #1140000, processed 21895521 words and 2471791 word types\n",
      "2017-02-25 22:37:57,578 : INFO : PROGRESS: at sentence #1150000, processed 22095027 words and 2487389 word types\n",
      "2017-02-25 22:37:57,919 : INFO : PROGRESS: at sentence #1160000, processed 22278249 words and 2500292 word types\n",
      "2017-02-25 22:37:58,266 : INFO : PROGRESS: at sentence #1170000, processed 22461411 words and 2506964 word types\n",
      "2017-02-25 22:37:58,624 : INFO : PROGRESS: at sentence #1180000, processed 22646087 words and 2523353 word types\n",
      "2017-02-25 22:37:58,979 : INFO : PROGRESS: at sentence #1190000, processed 22831473 words and 2532285 word types\n",
      "2017-02-25 22:37:59,328 : INFO : PROGRESS: at sentence #1200000, processed 23024626 words and 2542560 word types\n",
      "2017-02-25 22:37:59,693 : INFO : PROGRESS: at sentence #1210000, processed 23217183 words and 2558354 word types\n",
      "2017-02-25 22:38:00,057 : INFO : PROGRESS: at sentence #1220000, processed 23410112 words and 2571034 word types\n",
      "2017-02-25 22:38:00,404 : INFO : PROGRESS: at sentence #1230000, processed 23600890 words and 2577269 word types\n",
      "2017-02-25 22:38:00,765 : INFO : PROGRESS: at sentence #1240000, processed 23790632 words and 2586910 word types\n",
      "2017-02-25 22:38:01,127 : INFO : PROGRESS: at sentence #1250000, processed 23976933 words and 2601627 word types\n",
      "2017-02-25 22:38:01,477 : INFO : PROGRESS: at sentence #1260000, processed 24164783 words and 2615386 word types\n",
      "2017-02-25 22:38:01,830 : INFO : PROGRESS: at sentence #1270000, processed 24355143 words and 2634825 word types\n",
      "2017-02-25 22:38:02,205 : INFO : PROGRESS: at sentence #1280000, processed 24559908 words and 2651956 word types\n",
      "2017-02-25 22:38:02,540 : INFO : PROGRESS: at sentence #1290000, processed 24738873 words and 2666897 word types\n",
      "2017-02-25 22:38:02,863 : INFO : PROGRESS: at sentence #1300000, processed 24915230 words and 2680427 word types\n",
      "2017-02-25 22:38:03,233 : INFO : PROGRESS: at sentence #1310000, processed 25102644 words and 2695950 word types\n",
      "2017-02-25 22:38:03,621 : INFO : PROGRESS: at sentence #1320000, processed 25303207 words and 2712685 word types\n",
      "2017-02-25 22:38:04,006 : INFO : PROGRESS: at sentence #1330000, processed 25502993 words and 2729337 word types\n",
      "2017-02-25 22:38:04,296 : INFO : collected 2741164 word types from a corpus of 25654735 words (unigram + bigrams) and 1337771 sentences\n",
      "2017-02-25 22:38:04,297 : INFO : using 2741164 counts as vocab in Phrases<0 vocab, min_count=2, threshold=10.0, max_vocab_size=40000000>\n",
      "2017-02-25 22:38:04,493 : INFO : source_vocab length 2741164\n",
      "2017-02-25 22:38:29,688 : INFO : Phraser added 50000 phrasegrams\n",
      "2017-02-25 22:38:29,778 : INFO : Phraser built with 50162 50162 phrasegrams\n",
      "2017-02-25 22:39:49,701 : INFO : collecting all words and their counts\n",
      "2017-02-25 22:39:49,701 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-25 22:39:49,744 : INFO : PROGRESS: at sentence #10000, processed 190213 words, keeping 15543 word types\n",
      "2017-02-25 22:39:49,794 : INFO : PROGRESS: at sentence #20000, processed 382317 words, keeping 23056 word types\n",
      "2017-02-25 22:39:49,847 : INFO : PROGRESS: at sentence #30000, processed 572352 words, keeping 28430 word types\n",
      "2017-02-25 22:39:49,901 : INFO : PROGRESS: at sentence #40000, processed 761647 words, keeping 32830 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-25 22:39:49,948 : INFO : PROGRESS: at sentence #50000, processed 951475 words, keeping 36663 word types\n",
      "2017-02-25 22:39:49,998 : INFO : PROGRESS: at sentence #60000, processed 1145852 words, keeping 40397 word types\n",
      "2017-02-25 22:39:50,047 : INFO : PROGRESS: at sentence #70000, processed 1336930 words, keeping 43417 word types\n",
      "2017-02-25 22:39:50,101 : INFO : PROGRESS: at sentence #80000, processed 1530325 words, keeping 46399 word types\n",
      "2017-02-25 22:39:50,153 : INFO : PROGRESS: at sentence #90000, processed 1724966 words, keeping 49218 word types\n",
      "2017-02-25 22:39:50,201 : INFO : PROGRESS: at sentence #100000, processed 1913391 words, keeping 51577 word types\n",
      "2017-02-25 22:39:50,250 : INFO : PROGRESS: at sentence #110000, processed 2104073 words, keeping 53852 word types\n",
      "2017-02-25 22:39:50,304 : INFO : PROGRESS: at sentence #120000, processed 2302458 words, keeping 56087 word types\n",
      "2017-02-25 22:39:50,355 : INFO : PROGRESS: at sentence #130000, processed 2493938 words, keeping 57995 word types\n",
      "2017-02-25 22:39:50,407 : INFO : PROGRESS: at sentence #140000, processed 2684334 words, keeping 59812 word types\n",
      "2017-02-25 22:39:50,459 : INFO : PROGRESS: at sentence #150000, processed 2875324 words, keeping 61651 word types\n",
      "2017-02-25 22:39:50,507 : INFO : PROGRESS: at sentence #160000, processed 3063538 words, keeping 63411 word types\n",
      "2017-02-25 22:39:50,558 : INFO : PROGRESS: at sentence #170000, processed 3255193 words, keeping 65117 word types\n",
      "2017-02-25 22:39:50,611 : INFO : PROGRESS: at sentence #180000, processed 3451960 words, keeping 66839 word types\n",
      "2017-02-25 22:39:50,660 : INFO : PROGRESS: at sentence #190000, processed 3640654 words, keeping 68326 word types\n",
      "2017-02-25 22:39:50,707 : INFO : PROGRESS: at sentence #200000, processed 3827630 words, keeping 69755 word types\n",
      "2017-02-25 22:39:50,764 : INFO : PROGRESS: at sentence #210000, processed 4021791 words, keeping 71250 word types\n",
      "2017-02-25 22:39:50,816 : INFO : PROGRESS: at sentence #220000, processed 4217839 words, keeping 72726 word types\n",
      "2017-02-25 22:39:50,868 : INFO : PROGRESS: at sentence #230000, processed 4413721 words, keeping 74221 word types\n",
      "2017-02-25 22:39:50,924 : INFO : PROGRESS: at sentence #240000, processed 4599867 words, keeping 75501 word types\n",
      "2017-02-25 22:39:50,973 : INFO : PROGRESS: at sentence #250000, processed 4795655 words, keeping 77000 word types\n",
      "2017-02-25 22:39:51,028 : INFO : PROGRESS: at sentence #260000, processed 4986610 words, keeping 78592 word types\n",
      "2017-02-25 22:39:51,074 : INFO : PROGRESS: at sentence #270000, processed 5176542 words, keeping 80040 word types\n",
      "2017-02-25 22:39:51,124 : INFO : PROGRESS: at sentence #280000, processed 5366035 words, keeping 81425 word types\n",
      "2017-02-25 22:39:51,176 : INFO : PROGRESS: at sentence #290000, processed 5554428 words, keeping 82811 word types\n",
      "2017-02-25 22:39:51,228 : INFO : PROGRESS: at sentence #300000, processed 5744705 words, keeping 84120 word types\n",
      "2017-02-25 22:39:51,277 : INFO : PROGRESS: at sentence #310000, processed 5937099 words, keeping 85385 word types\n",
      "2017-02-25 22:39:51,324 : INFO : PROGRESS: at sentence #320000, processed 6126305 words, keeping 86585 word types\n",
      "2017-02-25 22:39:51,378 : INFO : PROGRESS: at sentence #330000, processed 6317668 words, keeping 87844 word types\n",
      "2017-02-25 22:39:51,425 : INFO : PROGRESS: at sentence #340000, processed 6507139 words, keeping 88991 word types\n",
      "2017-02-25 22:39:51,477 : INFO : PROGRESS: at sentence #350000, processed 6701588 words, keeping 90063 word types\n",
      "2017-02-25 22:39:51,526 : INFO : PROGRESS: at sentence #360000, processed 6891173 words, keeping 91135 word types\n",
      "2017-02-25 22:39:51,568 : INFO : PROGRESS: at sentence #370000, processed 7077813 words, keeping 92135 word types\n",
      "2017-02-25 22:39:51,614 : INFO : PROGRESS: at sentence #380000, processed 7266115 words, keeping 93047 word types\n",
      "2017-02-25 22:39:51,665 : INFO : PROGRESS: at sentence #390000, processed 7455029 words, keeping 94058 word types\n",
      "2017-02-25 22:39:51,715 : INFO : PROGRESS: at sentence #400000, processed 7645635 words, keeping 95088 word types\n",
      "2017-02-25 22:39:51,762 : INFO : PROGRESS: at sentence #410000, processed 7832618 words, keeping 96034 word types\n",
      "2017-02-25 22:39:51,811 : INFO : PROGRESS: at sentence #420000, processed 8025916 words, keeping 96963 word types\n",
      "2017-02-25 22:39:51,860 : INFO : PROGRESS: at sentence #430000, processed 8216285 words, keeping 97911 word types\n",
      "2017-02-25 22:39:51,907 : INFO : PROGRESS: at sentence #440000, processed 8405005 words, keeping 98798 word types\n",
      "2017-02-25 22:39:51,959 : INFO : PROGRESS: at sentence #450000, processed 8598226 words, keeping 99581 word types\n",
      "2017-02-25 22:39:52,006 : INFO : PROGRESS: at sentence #460000, processed 8786515 words, keeping 100456 word types\n",
      "2017-02-25 22:39:52,052 : INFO : PROGRESS: at sentence #470000, processed 8973024 words, keeping 101247 word types\n",
      "2017-02-25 22:39:52,109 : INFO : PROGRESS: at sentence #480000, processed 9170167 words, keeping 101694 word types\n",
      "2017-02-25 22:39:52,160 : INFO : PROGRESS: at sentence #490000, processed 9371952 words, keeping 101967 word types\n",
      "2017-02-25 22:39:52,211 : INFO : PROGRESS: at sentence #500000, processed 9561643 words, keeping 102404 word types\n",
      "2017-02-25 22:39:52,261 : INFO : PROGRESS: at sentence #510000, processed 9751864 words, keeping 102533 word types\n",
      "2017-02-25 22:39:52,314 : INFO : PROGRESS: at sentence #520000, processed 9958986 words, keeping 103497 word types\n",
      "2017-02-25 22:39:52,364 : INFO : PROGRESS: at sentence #530000, processed 10159342 words, keeping 104250 word types\n",
      "2017-02-25 22:39:52,411 : INFO : PROGRESS: at sentence #540000, processed 10345928 words, keeping 104836 word types\n",
      "2017-02-25 22:39:52,466 : INFO : PROGRESS: at sentence #550000, processed 10546302 words, keeping 105386 word types\n",
      "2017-02-25 22:39:52,515 : INFO : PROGRESS: at sentence #560000, processed 10723048 words, keeping 105994 word types\n",
      "2017-02-25 22:39:52,561 : INFO : PROGRESS: at sentence #570000, processed 10903425 words, keeping 106451 word types\n",
      "2017-02-25 22:39:52,605 : INFO : PROGRESS: at sentence #580000, processed 11093108 words, keeping 106894 word types\n",
      "2017-02-25 22:39:52,649 : INFO : PROGRESS: at sentence #590000, processed 11283959 words, keeping 107456 word types\n",
      "2017-02-25 22:39:52,708 : INFO : PROGRESS: at sentence #600000, processed 11489682 words, keeping 108008 word types\n",
      "2017-02-25 22:39:52,756 : INFO : PROGRESS: at sentence #610000, processed 11683667 words, keeping 108676 word types\n",
      "2017-02-25 22:39:52,798 : INFO : PROGRESS: at sentence #620000, processed 11861604 words, keeping 108826 word types\n",
      "2017-02-25 22:39:52,847 : INFO : PROGRESS: at sentence #630000, processed 12054149 words, keeping 109364 word types\n",
      "2017-02-25 22:39:52,893 : INFO : PROGRESS: at sentence #640000, processed 12235246 words, keeping 110085 word types\n",
      "2017-02-25 22:39:52,935 : INFO : PROGRESS: at sentence #650000, processed 12409330 words, keeping 110597 word types\n",
      "2017-02-25 22:39:52,983 : INFO : PROGRESS: at sentence #660000, processed 12593899 words, keeping 110759 word types\n",
      "2017-02-25 22:39:53,031 : INFO : PROGRESS: at sentence #670000, processed 12785402 words, keeping 111511 word types\n",
      "2017-02-25 22:39:53,082 : INFO : PROGRESS: at sentence #680000, processed 12976148 words, keeping 112120 word types\n",
      "2017-02-25 22:39:53,131 : INFO : PROGRESS: at sentence #690000, processed 13182068 words, keeping 112771 word types\n",
      "2017-02-25 22:39:53,176 : INFO : PROGRESS: at sentence #700000, processed 13366643 words, keeping 113303 word types\n",
      "2017-02-25 22:39:53,224 : INFO : PROGRESS: at sentence #710000, processed 13563777 words, keeping 113782 word types\n",
      "2017-02-25 22:39:53,266 : INFO : PROGRESS: at sentence #720000, processed 13741481 words, keeping 114608 word types\n",
      "2017-02-25 22:39:53,315 : INFO : PROGRESS: at sentence #730000, processed 13928030 words, keeping 115385 word types\n",
      "2017-02-25 22:39:53,367 : INFO : PROGRESS: at sentence #740000, processed 14127009 words, keeping 115712 word types\n",
      "2017-02-25 22:39:53,414 : INFO : PROGRESS: at sentence #750000, processed 14320772 words, keeping 116123 word types\n",
      "2017-02-25 22:39:53,463 : INFO : PROGRESS: at sentence #760000, processed 14499991 words, keeping 116618 word types\n",
      "2017-02-25 22:39:53,513 : INFO : PROGRESS: at sentence #770000, processed 14688923 words, keeping 117175 word types\n",
      "2017-02-25 22:39:53,561 : INFO : PROGRESS: at sentence #780000, processed 14875879 words, keeping 117805 word types\n",
      "2017-02-25 22:39:53,613 : INFO : PROGRESS: at sentence #790000, processed 15068464 words, keeping 118075 word types\n",
      "2017-02-25 22:39:53,658 : INFO : PROGRESS: at sentence #800000, processed 15248082 words, keeping 118549 word types\n",
      "2017-02-25 22:39:53,705 : INFO : PROGRESS: at sentence #810000, processed 15435067 words, keeping 118965 word types\n",
      "2017-02-25 22:39:53,755 : INFO : PROGRESS: at sentence #820000, processed 15637825 words, keeping 119636 word types\n",
      "2017-02-25 22:39:53,801 : INFO : PROGRESS: at sentence #830000, processed 15820535 words, keeping 120411 word types\n",
      "2017-02-25 22:39:53,849 : INFO : PROGRESS: at sentence #840000, processed 16005345 words, keeping 121256 word types\n",
      "2017-02-25 22:39:53,900 : INFO : PROGRESS: at sentence #850000, processed 16198869 words, keeping 122117 word types\n",
      "2017-02-25 22:39:53,949 : INFO : PROGRESS: at sentence #860000, processed 16393521 words, keeping 122801 word types\n",
      "2017-02-25 22:39:53,994 : INFO : PROGRESS: at sentence #870000, processed 16581287 words, keeping 123508 word types\n",
      "2017-02-25 22:39:54,044 : INFO : PROGRESS: at sentence #880000, processed 16772228 words, keeping 124271 word types\n",
      "2017-02-25 22:39:54,087 : INFO : PROGRESS: at sentence #890000, processed 16958263 words, keeping 124916 word types\n",
      "2017-02-25 22:39:54,136 : INFO : PROGRESS: at sentence #900000, processed 17158437 words, keeping 125488 word types\n",
      "2017-02-25 22:39:54,186 : INFO : PROGRESS: at sentence #910000, processed 17355238 words, keeping 126116 word types\n",
      "2017-02-25 22:39:54,239 : INFO : PROGRESS: at sentence #920000, processed 17559657 words, keeping 126878 word types\n",
      "2017-02-25 22:39:54,295 : INFO : PROGRESS: at sentence #930000, processed 17757862 words, keeping 127310 word types\n",
      "2017-02-25 22:39:54,342 : INFO : PROGRESS: at sentence #940000, processed 17947141 words, keeping 127467 word types\n",
      "2017-02-25 22:39:54,393 : INFO : PROGRESS: at sentence #950000, processed 18137851 words, keeping 127647 word types\n",
      "2017-02-25 22:39:54,440 : INFO : PROGRESS: at sentence #960000, processed 18325871 words, keeping 127776 word types\n",
      "2017-02-25 22:39:54,491 : INFO : PROGRESS: at sentence #970000, processed 18520820 words, keeping 128183 word types\n",
      "2017-02-25 22:39:54,539 : INFO : PROGRESS: at sentence #980000, processed 18711889 words, keeping 128309 word types\n",
      "2017-02-25 22:39:54,592 : INFO : PROGRESS: at sentence #990000, processed 18921956 words, keeping 128571 word types\n",
      "2017-02-25 22:39:54,641 : INFO : PROGRESS: at sentence #1000000, processed 19114043 words, keeping 128924 word types\n",
      "2017-02-25 22:39:54,689 : INFO : PROGRESS: at sentence #1010000, processed 19312710 words, keeping 129181 word types\n",
      "2017-02-25 22:39:54,740 : INFO : PROGRESS: at sentence #1020000, processed 19502527 words, keeping 129614 word types\n",
      "2017-02-25 22:39:54,789 : INFO : PROGRESS: at sentence #1030000, processed 19691635 words, keeping 129993 word types\n",
      "2017-02-25 22:39:54,834 : INFO : PROGRESS: at sentence #1040000, processed 19883488 words, keeping 130382 word types\n",
      "2017-02-25 22:39:54,886 : INFO : PROGRESS: at sentence #1050000, processed 20083573 words, keeping 130798 word types\n",
      "2017-02-25 22:39:54,933 : INFO : PROGRESS: at sentence #1060000, processed 20268011 words, keeping 130955 word types\n",
      "2017-02-25 22:39:54,988 : INFO : PROGRESS: at sentence #1070000, processed 20473968 words, keeping 131327 word types\n",
      "2017-02-25 22:39:55,036 : INFO : PROGRESS: at sentence #1080000, processed 20660160 words, keeping 131766 word types\n",
      "2017-02-25 22:39:55,082 : INFO : PROGRESS: at sentence #1090000, processed 20838589 words, keeping 131873 word types\n",
      "2017-02-25 22:39:55,127 : INFO : PROGRESS: at sentence #1100000, processed 21021547 words, keeping 132320 word types\n",
      "2017-02-25 22:39:55,177 : INFO : PROGRESS: at sentence #1110000, processed 21200555 words, keeping 132906 word types\n",
      "2017-02-25 22:39:55,228 : INFO : PROGRESS: at sentence #1120000, processed 21383140 words, keeping 133334 word types\n",
      "2017-02-25 22:39:55,279 : INFO : PROGRESS: at sentence #1130000, processed 21570633 words, keeping 133697 word types\n",
      "2017-02-25 22:39:55,327 : INFO : PROGRESS: at sentence #1140000, processed 21764769 words, keeping 134059 word types\n",
      "2017-02-25 22:39:55,378 : INFO : PROGRESS: at sentence #1150000, processed 21961329 words, keeping 134359 word types\n",
      "2017-02-25 22:39:55,427 : INFO : PROGRESS: at sentence #1160000, processed 22144676 words, keeping 134699 word types\n",
      "2017-02-25 22:39:55,475 : INFO : PROGRESS: at sentence #1170000, processed 22328606 words, keeping 134903 word types\n",
      "2017-02-25 22:39:55,524 : INFO : PROGRESS: at sentence #1180000, processed 22514679 words, keeping 135603 word types\n",
      "2017-02-25 22:39:55,571 : INFO : PROGRESS: at sentence #1190000, processed 22700060 words, keeping 135843 word types\n",
      "2017-02-25 22:39:55,621 : INFO : PROGRESS: at sentence #1200000, processed 22893419 words, keeping 136092 word types\n",
      "2017-02-25 22:39:55,671 : INFO : PROGRESS: at sentence #1210000, processed 23085521 words, keeping 136543 word types\n",
      "2017-02-25 22:39:55,717 : INFO : PROGRESS: at sentence #1220000, processed 23278037 words, keeping 136933 word types\n",
      "2017-02-25 22:39:55,765 : INFO : PROGRESS: at sentence #1230000, processed 23468675 words, keeping 137108 word types\n",
      "2017-02-25 22:39:55,816 : INFO : PROGRESS: at sentence #1240000, processed 23658131 words, keeping 137343 word types\n",
      "2017-02-25 22:39:55,864 : INFO : PROGRESS: at sentence #1250000, processed 23845062 words, keeping 137721 word types\n",
      "2017-02-25 22:39:55,910 : INFO : PROGRESS: at sentence #1260000, processed 24032295 words, keeping 138088 word types\n",
      "2017-02-25 22:39:55,960 : INFO : PROGRESS: at sentence #1270000, processed 24222416 words, keeping 138554 word types\n",
      "2017-02-25 22:39:56,012 : INFO : PROGRESS: at sentence #1280000, processed 24425830 words, keeping 138988 word types\n",
      "2017-02-25 22:39:56,057 : INFO : PROGRESS: at sentence #1290000, processed 24605836 words, keeping 139364 word types\n",
      "2017-02-25 22:39:56,100 : INFO : PROGRESS: at sentence #1300000, processed 24782980 words, keeping 139717 word types\n",
      "2017-02-25 22:39:56,151 : INFO : PROGRESS: at sentence #1310000, processed 24970234 words, keeping 140155 word types\n",
      "2017-02-25 22:39:56,202 : INFO : PROGRESS: at sentence #1320000, processed 25170252 words, keeping 140596 word types\n",
      "2017-02-25 22:39:56,251 : INFO : PROGRESS: at sentence #1330000, processed 25369692 words, keeping 140996 word types\n",
      "2017-02-25 22:39:56,289 : INFO : collected 141383 word types from a corpus of 25521429 raw words and 1337771 sentences\n",
      "2017-02-25 22:39:56,290 : INFO : Loading a fresh vocabulary\n",
      "2017-02-25 22:39:56,416 : INFO : min_count=40 retains 18070 unique words (12% of original 141383, drops 123313)\n",
      "2017-02-25 22:39:56,416 : INFO : min_count=40 leaves 24764433 word corpus (97% of original 25521429, drops 756996)\n",
      "2017-02-25 22:39:56,466 : INFO : deleting the raw counts dictionary of 141383 items\n",
      "2017-02-25 22:39:56,474 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-02-25 22:39:56,474 : INFO : downsampling leaves estimated 17846175 word corpus (72.1% of prior 24764433)\n",
      "2017-02-25 22:39:56,475 : INFO : estimated required memory for 18070 words and 300 dimensions: 52403000 bytes\n",
      "2017-02-25 22:39:56,548 : INFO : resetting layer weights\n",
      "2017-02-25 22:39:56,833 : INFO : training model with 4 workers on 18070 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-25 22:39:56,833 : INFO : expecting 1337771 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-25 22:39:57,843 : INFO : PROGRESS: at 1.26% examples, 1118000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:39:58,844 : INFO : PROGRESS: at 2.55% examples, 1136678 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:39:59,845 : INFO : PROGRESS: at 3.83% examples, 1139826 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:00,845 : INFO : PROGRESS: at 5.11% examples, 1141649 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:01,852 : INFO : PROGRESS: at 6.42% examples, 1143947 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:02,857 : INFO : PROGRESS: at 7.68% examples, 1141124 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:03,860 : INFO : PROGRESS: at 8.96% examples, 1142656 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:04,861 : INFO : PROGRESS: at 10.26% examples, 1142413 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:05,864 : INFO : PROGRESS: at 11.57% examples, 1143131 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:06,866 : INFO : PROGRESS: at 12.88% examples, 1144257 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:07,866 : INFO : PROGRESS: at 14.14% examples, 1143779 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:08,869 : INFO : PROGRESS: at 15.36% examples, 1141182 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:09,869 : INFO : PROGRESS: at 16.61% examples, 1138850 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:10,875 : INFO : PROGRESS: at 17.89% examples, 1137445 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:11,880 : INFO : PROGRESS: at 19.10% examples, 1133690 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:12,881 : INFO : PROGRESS: at 20.32% examples, 1130191 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:13,882 : INFO : PROGRESS: at 21.53% examples, 1127551 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:14,890 : INFO : PROGRESS: at 22.71% examples, 1123198 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:15,897 : INFO : PROGRESS: at 23.94% examples, 1121206 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:16,901 : INFO : PROGRESS: at 25.16% examples, 1119875 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:17,908 : INFO : PROGRESS: at 26.37% examples, 1117100 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:18,913 : INFO : PROGRESS: at 27.55% examples, 1114067 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:19,920 : INFO : PROGRESS: at 28.76% examples, 1112717 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:20,926 : INFO : PROGRESS: at 30.00% examples, 1111360 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:21,927 : INFO : PROGRESS: at 31.22% examples, 1110377 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:22,931 : INFO : PROGRESS: at 32.48% examples, 1109681 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:23,940 : INFO : PROGRESS: at 33.70% examples, 1109244 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:24,941 : INFO : PROGRESS: at 34.89% examples, 1108380 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:25,945 : INFO : PROGRESS: at 36.07% examples, 1106896 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:26,951 : INFO : PROGRESS: at 37.31% examples, 1106043 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:27,958 : INFO : PROGRESS: at 38.40% examples, 1101221 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:28,962 : INFO : PROGRESS: at 39.65% examples, 1101203 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:29,965 : INFO : PROGRESS: at 40.88% examples, 1101261 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:30,966 : INFO : PROGRESS: at 42.10% examples, 1100991 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:31,967 : INFO : PROGRESS: at 43.04% examples, 1093377 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-25 22:40:32,969 : INFO : PROGRESS: at 44.14% examples, 1090225 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:33,970 : INFO : PROGRESS: at 45.18% examples, 1086095 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:34,979 : INFO : PROGRESS: at 46.36% examples, 1084699 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:35,988 : INFO : PROGRESS: at 47.40% examples, 1080839 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:36,997 : INFO : PROGRESS: at 48.27% examples, 1073436 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:38,002 : INFO : PROGRESS: at 49.41% examples, 1071629 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:39,006 : INFO : PROGRESS: at 50.41% examples, 1066870 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:40,011 : INFO : PROGRESS: at 51.39% examples, 1061930 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:41,012 : INFO : PROGRESS: at 52.34% examples, 1057052 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:42,016 : INFO : PROGRESS: at 53.41% examples, 1054209 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:43,021 : INFO : PROGRESS: at 54.54% examples, 1053694 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:44,024 : INFO : PROGRESS: at 55.71% examples, 1054204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:45,030 : INFO : PROGRESS: at 56.85% examples, 1052782 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:46,035 : INFO : PROGRESS: at 58.00% examples, 1051991 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:47,045 : INFO : PROGRESS: at 59.08% examples, 1050101 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:48,052 : INFO : PROGRESS: at 60.12% examples, 1047511 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:49,058 : INFO : PROGRESS: at 61.24% examples, 1046518 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:50,064 : INFO : PROGRESS: at 62.35% examples, 1045440 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:51,070 : INFO : PROGRESS: at 63.43% examples, 1043868 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:52,073 : INFO : PROGRESS: at 64.67% examples, 1044902 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:53,074 : INFO : PROGRESS: at 65.90% examples, 1045697 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:54,076 : INFO : PROGRESS: at 67.09% examples, 1046050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:55,083 : INFO : PROGRESS: at 68.30% examples, 1046879 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:56,083 : INFO : PROGRESS: at 69.54% examples, 1047678 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:57,086 : INFO : PROGRESS: at 70.79% examples, 1048312 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:58,087 : INFO : PROGRESS: at 72.00% examples, 1048600 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:40:59,088 : INFO : PROGRESS: at 73.22% examples, 1049233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:00,093 : INFO : PROGRESS: at 74.43% examples, 1049905 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:01,098 : INFO : PROGRESS: at 75.62% examples, 1050411 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:02,104 : INFO : PROGRESS: at 76.78% examples, 1049836 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:03,111 : INFO : PROGRESS: at 77.83% examples, 1047778 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:04,115 : INFO : PROGRESS: at 78.91% examples, 1046542 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:05,122 : INFO : PROGRESS: at 79.97% examples, 1044972 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:06,124 : INFO : PROGRESS: at 81.09% examples, 1044217 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:07,133 : INFO : PROGRESS: at 82.23% examples, 1043781 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:08,140 : INFO : PROGRESS: at 83.34% examples, 1042987 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:09,143 : INFO : PROGRESS: at 84.45% examples, 1042322 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:10,148 : INFO : PROGRESS: at 85.48% examples, 1040544 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:11,150 : INFO : PROGRESS: at 86.55% examples, 1039326 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:12,152 : INFO : PROGRESS: at 87.75% examples, 1039854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:13,156 : INFO : PROGRESS: at 88.98% examples, 1040769 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:14,156 : INFO : PROGRESS: at 90.28% examples, 1041986 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:15,158 : INFO : PROGRESS: at 91.59% examples, 1043282 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:16,162 : INFO : PROGRESS: at 92.85% examples, 1044224 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:17,170 : INFO : PROGRESS: at 94.09% examples, 1045045 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:18,174 : INFO : PROGRESS: at 95.34% examples, 1046115 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:19,183 : INFO : PROGRESS: at 96.65% examples, 1047400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:20,184 : INFO : PROGRESS: at 97.91% examples, 1048149 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:21,187 : INFO : PROGRESS: at 99.15% examples, 1048978 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-25 22:41:21,851 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-25 22:41:21,855 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-25 22:41:21,860 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-25 22:41:21,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-25 22:41:21,862 : INFO : training on 127607145 raw words (89229939 effective words) took 85.0s, 1049455 effective words/s\n",
      "2017-02-25 22:41:21,863 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-02-25 22:41:21,994 : INFO : saving Word2Vec object under 300features_40minwords_10context_bigram, separately None\n",
      "2017-02-25 22:41:21,995 : INFO : not storing attribute syn0norm\n",
      "2017-02-25 22:41:21,996 : INFO : not storing attribute cum_table\n",
      "2017-02-25 22:41:22,516 : INFO : saved 300features_40minwords_10context_bigram\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "biigram word2vec\n",
    "'''\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import phrases\n",
    "from gensim.models import word2vec\n",
    "\n",
    "bigram = Phrases(sentences,min_count=2)\n",
    "\n",
    "bigram_phraser = phrases.Phraser(bigram)\n",
    "tokens_ = bigram_phraser[sentences]\n",
    "all_words = [j for j in tokens_ ]\n",
    "\n",
    "print('Training model...\\n')\n",
    "\n",
    "modelBigram = word2vec.Word2Vec(all_words,workers=num_workers,size=num_features,min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "modelBigram.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name_2 = \"300features_40minwords_10context_bigram\"\n",
    "modelBigram.save(model_name_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cup'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBigram.doesnt_match(\"man woman child cup\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 34092\n",
      "Review 1000 of 34092\n",
      "Review 2000 of 34092\n",
      "Review 3000 of 34092\n",
      "Review 4000 of 34092\n",
      "Review 5000 of 34092\n",
      "Review 6000 of 34092\n",
      "Review 7000 of 34092\n",
      "Review 8000 of 34092\n",
      "Review 9000 of 34092\n",
      "Review 10000 of 34092\n",
      "Review 11000 of 34092\n",
      "Review 12000 of 34092\n",
      "Review 13000 of 34092\n",
      "Review 14000 of 34092\n",
      "Review 15000 of 34092\n",
      "Review 16000 of 34092\n",
      "Review 17000 of 34092\n",
      "Review 18000 of 34092\n",
      "Review 19000 of 34092\n",
      "Review 20000 of 34092\n",
      "Review 21000 of 34092\n",
      "Review 22000 of 34092\n",
      "Review 23000 of 34092\n",
      "Review 24000 of 34092\n",
      "Review 25000 of 34092\n",
      "Review 26000 of 34092\n",
      "Review 27000 of 34092\n",
      "Review 28000 of 34092\n",
      "Review 29000 of 34092\n",
      "Review 30000 of 34092\n",
      "Review 31000 of 34092\n",
      "Review 32000 of 34092\n",
      "Review 33000 of 34092\n",
      "Review 34000 of 34092\n",
      "Review 0 of 14613\n",
      "Review 1000 of 14613\n",
      "Review 2000 of 14613\n",
      "Review 3000 of 14613\n",
      "Review 4000 of 14613\n",
      "Review 5000 of 14613\n",
      "Review 6000 of 14613\n",
      "Review 7000 of 14613\n",
      "Review 8000 of 14613\n",
      "Review 9000 of 14613\n",
      "Review 10000 of 14613\n",
      "Review 11000 of 14613\n",
      "Review 12000 of 14613\n",
      "Review 13000 of 14613\n",
      "Review 14000 of 14613\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, modelBigram, num_features )\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, modelBigram, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 355.6457359790802 seconds..\n",
      "\n",
      "Accuracy score =  0.79066584548 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.81      0.73      0.77      6862\n",
      "   negative       0.78      0.84      0.81      7751\n",
      "\n",
      "avg / total       0.79      0.79      0.79     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "bigram svm kernel linear \\ wordtovec  300 \\ remove stop word\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "\n",
    "tic()\n",
    "classifier_linear_svc = SVC(kernel='linear')\n",
    "classifier_linear_svc.fit(trainDataVecs, train['sentiment'])\n",
    "prediction_linear_svc= classifier_linear_svc.predict(testDataVecs)\n",
    "toc()\n",
    "\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], prediction_linear_svc),'..\\n')\n",
    "print(classification_report(test['sentiment'], prediction_linear_svc , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty =  l2  ..\n",
      "\n",
      "Accuracy score = 0.799356737152 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.81      0.76      0.78      6862\n",
      "   negative       0.79      0.84      0.82      7751\n",
      "\n",
      "avg / total       0.80      0.80      0.80     14613\n",
      "\n",
      "penalty =  l1  ..\n",
      "\n",
      "Accuracy score = 0.799767330459 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.80      0.76      0.78      6862\n",
      "   negative       0.80      0.84      0.82      7751\n",
      "\n",
      "avg / total       0.80      0.80      0.80     14613\n",
      "\n",
      "Elapsed time is 40.95446801185608 seconds..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "bigram svm linear \\ wordtovec  300 \\ didt remove stop word\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "tic()\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    classifier_linear = LinearSVC(penalty = penalty,dual = False)\n",
    "    classifier_linear.fit(trainDataVecs, train['sentiment'])\n",
    "    prediction_linear = classifier_linear.predict(testDataVecs)\n",
    "    print('penalty = ',penalty,' ..\\n')\n",
    "    print('Accuracy score =',accuracy_score(test['sentiment'], prediction_linear),'..\\n')\n",
    "    print(classification_report(test['sentiment'], prediction_linear , target_names=target_names))\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 360.3845491409302 seconds..\n",
      "\n",
      "Accuracy score =  0.790460548826 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.81      0.73      0.76      6862\n",
      "   negative       0.78      0.85      0.81      7751\n",
      "\n",
      "avg / total       0.79      0.79      0.79     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "unigram svm kernel lineqr \\ wordtovec  300 \\ remove stop word\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "\n",
    "tic()\n",
    "classifier_linear_svc = SVC(kernel='linear')\n",
    "classifier_linear_svc.fit(trainDataVecs, train['sentiment'])\n",
    "prediction_linear_svc= classifier_linear_svc.predict(testDataVecs)\n",
    "toc()\n",
    "\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], prediction_linear_svc),'..\\n')\n",
    "print(classification_report(test['sentiment'], prediction_linear_svc , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty =  l2  ..\n",
      "\n",
      "Accuracy score = 0.793950591939 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.80      0.75      0.77      6862\n",
      "   negative       0.79      0.83      0.81      7751\n",
      "\n",
      "avg / total       0.79      0.79      0.79     14613\n",
      "\n",
      "penalty =  l1  ..\n",
      "\n",
      "Accuracy score = 0.798330253884 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.80      0.76      0.78      6862\n",
      "   negative       0.80      0.83      0.81      7751\n",
      "\n",
      "avg / total       0.80      0.80      0.80     14613\n",
      "\n",
      "Elapsed time is 29.16849398612976 seconds..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "unigram svm linear \\ wordtovec  300 \\ didt remove stop word\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "tic()\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    classifier_linear = LinearSVC(penalty = penalty,dual = False)\n",
    "    classifier_linear.fit(trainDataVecs, train['sentiment'])\n",
    "    prediction_linear = classifier_linear.predict(testDataVecs)\n",
    "    print('penalty = ',penalty,' ..\\n')\n",
    "    print('Accuracy score =',accuracy_score(test['sentiment'], prediction_linear),'..\\n')\n",
    "    print(classification_report(test['sentiment'], prediction_linear , target_names=target_names))\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty =  l2  ..\n",
      "\n",
      "Accuracy score = 0.797509067269 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.80      0.75      0.78      6862\n",
      "   negative       0.79      0.84      0.81      7751\n",
      "\n",
      "avg / total       0.80      0.80      0.80     14613\n",
      "\n",
      "penalty =  l1  ..\n",
      "\n",
      "Accuracy score = 0.801204407035 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.81      0.76      0.78      6862\n",
      "   negative       0.80      0.84      0.82      7751\n",
      "\n",
      "avg / total       0.80      0.80      0.80     14613\n",
      "\n",
      "Elapsed time is 38.775890827178955 seconds..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "unigram svm linear \\ wordtovec  300 \\ remove stop word\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "tic()\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    classifier_linear_reStop = LinearSVC(penalty = penalty,dual = False)\n",
    "    classifier_linear_reStop.fit(trainDataVecs, train['sentiment'])\n",
    "    prediction_linear_reStop = classifier_linear_reStop.predict(testDataVecs)\n",
    "    print('penalty = ',penalty,' ..\\n')\n",
    "    print('Accuracy score =',accuracy_score(test['sentiment'], prediction_linear_reStop),'..\\n')\n",
    "    print(classification_report(test['sentiment'], prediction_linear_reStop , target_names=target_names))\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2225e784b898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training classifier...\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclassifier_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclassifier_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprediction_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDataVecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    585\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    586\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "#Naive bayes cant because of the negative value\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,roc_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tic()\n",
    "print('Training classifier...\\n')\n",
    "classifier_nb = MultinomialNB()\n",
    "classifier_nb.fit(trainDataVecs, train['sentiment'])\n",
    "prediction_nb = classifier.predict(testDataVecs)\n",
    "toc()\n",
    "\n",
    "print('Classifier: Naive Bayes, ','n-gram: ',min_gram,max_gram,'..\\n')\n",
    "\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], prediction_nb),'..\\n')\n",
    "\n",
    "print(classification_report(test['sentiment'], prediction_nb, target_names=target_names))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  359.9676048755646 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "bags of centroid\n",
    "'''\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['crawls', 'crawling', 'floats', 'inch']\n",
      "\n",
      "Cluster 1\n",
      "['duty', 'toll', 'stance']\n",
      "\n",
      "Cluster 2\n",
      "['blandness', 'unknowns', 'uniformly', 'selection']\n",
      "\n",
      "Cluster 3\n",
      "['echoing', 'chanting', 'screeching', 'swift', 'noises']\n",
      "\n",
      "Cluster 4\n",
      "['embrace', 'cling', 'dominate', 'restore', 'preserve', 'obtain', 'conform', 'gain', 'navigate', 'fulfil', 'regain', 'conceal', 'rule', 'rival', 'enrich', 'exploit', 'conquer', 'pursue', 'abandon', 'acquire']\n",
      "\n",
      "Cluster 5\n",
      "['hamm', 'cryer', 'bernthal']\n",
      "\n",
      "Cluster 6\n",
      "['oz', 'chocolate', 'willy', 'factory', 'wonka']\n",
      "\n",
      "Cluster 7\n",
      "['parenting', 'mistrust', 'importance', 'acknowledging', 'stability', 'selfishness', 'compass', 'overcoming', 'oneself', 'acceptance', 'accepting', 'affairs', 'discovery']\n",
      "\n",
      "Cluster 8\n",
      "['independently', 'arab', 'israeli']\n",
      "\n",
      "Cluster 9\n",
      "['predictable', 'clich', 'formulaic']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    num = list(word_centroid_map.values())\n",
    "    for i in range(0,len(num)):\n",
    "        if( num[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( list(word_centroid_map.values()) ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        for w in word:\n",
    "            if w in word_centroid_map:\n",
    "                index = word_centroid_map[w]\n",
    "                bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"reviews\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review,word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"reviews\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty =  l2  ..\n",
      "\n",
      "Accuracy score = 0.792376650927 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.79      0.76      0.77      6862\n",
      "   negative       0.79      0.82      0.81      7751\n",
      "\n",
      "avg / total       0.79      0.79      0.79     14613\n",
      "\n",
      "penalty =  l1  ..\n",
      "\n",
      "Accuracy score = 0.793539998631 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.79      0.76      0.78      6862\n",
      "   negative       0.79      0.82      0.81      7751\n",
      "\n",
      "avg / total       0.79      0.79      0.79     14613\n",
      "\n",
      "Elapsed time is 13.326980113983154 seconds..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "unigram svm linear \\ wordtovec  300 \\ remove stop word\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "tic()\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    classifier_kmean = LinearSVC(penalty = penalty,dual = False)\n",
    "    classifier_kmean.fit(train_centroids, train['sentiment'])\n",
    "    prediction_kmean = classifier_kmean.predict(test_centroids)\n",
    "    print('penalty = ',penalty,' ..\\n')\n",
    "    print('Accuracy score =',accuracy_score(test['sentiment'], prediction_kmean),'..\\n')\n",
    "    print(classification_report(test['sentiment'], prediction_kmean, target_names=target_names))\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 76.47679591178894 seconds..\n",
      "\n",
      "Accuracy score = 0.779374529529 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.79      0.73      0.76      6862\n",
      "   negative       0.77      0.83      0.80      7751\n",
      "\n",
      "avg / total       0.78      0.78      0.78     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "tic()\n",
    "forest = RandomForestClassifier(n_estimators = 200)\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "toc()\n",
    "print('Accuracy score =',accuracy_score(test['sentiment'], result),'..\\n')\n",
    "print(classification_report(test['sentiment'],result, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
