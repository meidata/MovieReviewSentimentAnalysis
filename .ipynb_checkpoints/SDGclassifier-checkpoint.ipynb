{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (34092, 3) ..\n",
      "\n",
      "test data size: (14613, 3) ..\n",
      "\n",
      "Unlabeled train data size : (60797, 2) ..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup \n",
    "import matplotlib.pyplot as plt\n",
    "from reviewsData import * \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "pd.set_option(\"display.max_colwidth\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(min_df=5, \n",
    "                                    max_df = 0.8,\n",
    "                                    sublinear_tf=True,\n",
    "                                    use_idf=True,\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=(1,2),\n",
    "                                    stop_words = None)\n",
    "\n",
    "\n",
    "\n",
    "train_vectors = tf_vectorizer.fit_transform(train['reviews'])\n",
    "test_vectors = tf_vectorizer.transform(test['reviews'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_stop = TfidfVectorizer(min_df=5, \n",
    "                                    max_df = 0.8,\n",
    "                                    sublinear_tf=True,\n",
    "                                    use_idf=True,\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=(1,2),\n",
    "                                    stop_words = 'english')\n",
    "\n",
    "train_vectors_stop = tf_stop.fit_transform(train['reviews'])\n",
    "test_vectors_stop = tf_stop.transform(test['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trigram\n",
    "tf = TfidfVectorizer(min_df=3, \n",
    "                                    max_df = 0.8,\n",
    "                                    sublinear_tf=True,\n",
    "                                    use_idf=True,\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=(1,3),\n",
    "                                    stop_words = None)\n",
    "\n",
    "train_vectors_tri = tf.fit_transform(train['reviews'])\n",
    "test_vectors_tri = tf.transform(test['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.84089509341 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.86      0.78      0.82      6862\n",
      "   negative       0.82      0.89      0.86      7751\n",
      "\n",
      "avg / total       0.84      0.84      0.84     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trigram\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "#f1_scorer = make_scorer(f1_score, pos_label=\"postive\",average='binary')\n",
    "precision_scorer = make_scorer(precision_score, pos_label=\"postive\")\n",
    "\n",
    "sgd_params = {'alpha': [0.00006, 0.00007, 0.00008, 0.0001, 0.0005,0.0003]} # Regularization parameter\n",
    "    \n",
    "model_SGD = GridSearchCV(SGD(random_state = 0, shuffle = True, penalty='l2',\n",
    "                         loss = 'hinge'), \n",
    "                         sgd_params, scoring = precision_scorer, cv = 30) # Find out which regularization parameter works the best. \n",
    "                            \n",
    "model_SGD.fit(train_vectors_tri, train['sentiment']) # Fit the model.\n",
    "SGD_result = model_SGD.predict(test_vectors_tri)\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.831725176213 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.86      0.77      0.81      6862\n",
      "   negative       0.81      0.89      0.85      7751\n",
      "\n",
      "avg / total       0.83      0.83      0.83     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stopword remove\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "#f1_scorer = make_scorer(f1_score, pos_label=\"postive\",average='binary')\n",
    "precision_scorer = make_scorer(precision_score, pos_label=\"postive\")\n",
    "\n",
    "sgd_params = {'alpha': [0.00006, 0.00007, 0.00008, 0.0001, 0.0005,0.0003]} # Regularization parameter\n",
    "    \n",
    "model_SGD = GridSearchCV(SGD(random_state = 0, shuffle = True, penalty='l2',\n",
    "                         loss = 'hinge'), \n",
    "                         sgd_params, scoring = precision_scorer, cv = 30) # Find out which regularization parameter works the best. \n",
    "                            \n",
    "model_SGD.fit(train_vectors_stop, train['sentiment']) # Fit the model.\n",
    "SGD_result = model_SGD.predict(test_vectors_stop)\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "#f1_scorer = make_scorer(f1_score, pos_label=\"postive\",average='binary')\n",
    "precision_scorer = make_scorer(precision_score, pos_label=\"postive\")\n",
    "\n",
    "sgd_params = {'alpha': [0.00006, 0.00007, 0.00008, 0.0001, 0.0005,0.0003]} # Regularization parameter\n",
    "    \n",
    "model_SGD = GridSearchCV(SGD(random_state = 0, shuffle = True, penalty='l2',\n",
    "                         loss = 'hinge'), \n",
    "                         sgd_params, scoring = precision_scorer, cv = 30) # Find out which regularization parameter works the best. \n",
    "                            \n",
    "model_SGD.fit(train_vectors, train['sentiment']) # Fit the model.\n",
    "SGD_result = model_SGD.predict(test_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.826524327653 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.85      0.77      0.81      6862\n",
      "   negative       0.81      0.87      0.84      7751\n",
      "\n",
      "avg / total       0.83      0.83      0.83     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM + penalty l1\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.842537466639 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.86      0.79      0.82      6862\n",
      "   negative       0.83      0.89      0.86      7751\n",
      "\n",
      "avg / total       0.84      0.84      0.84     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM + penalty l2\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.842537466639 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.86      0.79      0.82      6862\n",
      "   negative       0.83      0.89      0.86      7751\n",
      "\n",
      "avg / total       0.84      0.84      0.84     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM no specifique penatly (defaut l2)\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.837131321426 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.85      0.79      0.82      6862\n",
      "   negative       0.83      0.88      0.85      7751\n",
      "\n",
      "avg / total       0.84      0.84      0.84     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic Regression log penalty\n",
    "\n",
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.839184287963 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.85      0.80      0.82      6862\n",
      "   negative       0.83      0.88      0.85      7751\n",
      "\n",
      "avg / total       0.84      0.84      0.84     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.839115855745 ..\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    postive       0.85      0.79      0.82      6862\n",
      "   negative       0.83      0.88      0.85      7751\n",
      "\n",
      "avg / total       0.84      0.84      0.84     14613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['postive', 'negative']\n",
    "print('Accuracy score = ',accuracy_score(test['sentiment'], SGD_result ),'..\\n')\n",
    "print(classification_report(test['sentiment'], SGD_result , target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82686741684050813"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "precision_scorer = make_scorer(f1_score, pos_label=\"postive\")\n",
    "model_NB = MNB()\n",
    "model_NB.fit(train_vectors, train['sentiment'])\n",
    "SGD_result_NB = model_NB.predict(test_vectors)\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "np.mean(cross_val_score(model_NB,test_vectors,test['sentiment'],cv=20, scoring=precision_scorer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
